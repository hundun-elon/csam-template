\documentclass[a4paper,twoside,12pt]{report}
% Richard Klein (2020,2021)

% Include Packages
%\usepackage[a4paper,inner=3.5cm,outer=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}  % Set page margins
\usepackage{fullpage}
\usepackage{float}                  % Allows 'Here and Only Here' [H] for Floats
\usepackage{url}                    % \url{} command
\usepackage{charter}                  % Set font to Times
\usepackage{graphicx}               % \includegraphics
\usepackage{subfigure}              % Allow subfigures
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{parskip}
\usepackage[all]{nowidow}
\setnoclub[2]
\setnowidow[2]

% Referencing
% Provides \Vref and \vref to indicate where a reference is.
\usepackage{varioref} 
% Hyperlinks references
\usepackage[bookmarks=true,bookmarksopen=true]{hyperref} 
% Provides \Cref, \cref, \Vref, \vref to include the type of reference: fig/eqn/tbl
\usepackage{cleveref} 
% Setup Hyperref
\hypersetup{
  colorlinks   = true,              %Colours links instead of ugly boxes
  urlcolor     = blue,              %Colour for external hyperlinks
  linkcolor    = blue,              %Colour of internal links
  citecolor    = blue                %Colour of citations
}
% Names for Clever Ref
\crefname{table}{table}{tables}
\Crefname{table}{Table}{Tables}
\crefname{figure}{figure}{figures}
\Crefname{figure}{Figure}{Figures}
\crefname{equation}{equation}{equations}
\Crefname{equation}{Equation}{Equations}

% Wits Citation Style
\usepackage{natbib} \input{natbib-add}
\bibliographystyle{named-wits}
\bibpunct{[}{]}{;}{a}{}{}  % to get correct punctuation for bibliography
\setlength{\skip\footins}{1.5cm}
\newcommand{\citets}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\renewcommand\bibname{References}  

\pagestyle{headings}

\pagestyle{plain}
\pagenumbering{roman}

\renewenvironment{abstract}{\ \vfill\begin{center}\textbf{Abstract}\end{center}\addcontentsline{toc}{section}{Abstract}}{\vfill\vfill\newpage}
\newenvironment{declaration}{\ \vfill\begin{center}\textbf{Declaration}\end{center}\addcontentsline{toc}{section}{Declaration}}{\vfill\vfill\newpage}
\newenvironment{acknowledgements}{\ \vfill\begin{center}\textbf{Acknowledgements}\end{center}\addcontentsline{toc}{section}{Acknowledgements}}{\vfill\vfill\newpage}

\begin{document}
\onecolumn
\thispagestyle{empty}

\setcounter{page}{0}
\addcontentsline{toc}{chapter}{Preface}
\ 
\begin{center}
  \vfill
  {
  \huge \bf \textsc{Automated Grading of Free-Text Student Submissions Using Large Language Models.}\\
  \large Subtitle\\[20pt]
  \large School of Computer Science \& Applied Mathematics\\
  \large University of the Witwatersrand\\[20pt]
  \normalsize
  Sphamandla Mbuyazi\\
  2618115\\[20pt]
  Supervised by Prof. Richard Klein\\[10pt]
  \today
  }

  \vfill
  \vfill
  \includegraphics[width=1.5cm]{images/wits}
  \vspace{10pt}\\
  \small{Ethics Clearance Number: XX/XX/XX}\\[10pt]
  \small{A proposal submitted to the Faculty of Science, University of the Witwatersrand, Johannesburg,
in partial fulfilment of the requirements for the degree of Bachelor of Science with Honours}\\
\end{center}
\vfill
\newpage

\pagestyle{plain}
\setcounter{page}{1}

\phantomsection
\begin{abstract}
Abstract things....
\end{abstract}

\phantomsection
\begin{declaration}
I, Sphamandla Mbuyazi, hereby declare the contents of this research proposal to be my own work.
This proposal is submitted for the degree of Bachelor of Science with Honours in Computer Science at the University of the Witwatersrand.
This work has not been submitted to any other university, or for any other degree.
\end{declaration}

\phantomsection
\begin{acknowledgements}
Thanks World.
\end{acknowledgements}


\phantomsection
\addcontentsline{toc}{section}{Table of Contents}
\tableofcontents
\newpage
\phantomsection
\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\newpage
\phantomsection
\addcontentsline{toc}{section}{List of Tables}
\listoftables
\newpage
\pagenumbering{arabic}

\chapter{Introduction}
\section{Introduction}
% introduction paragraph.
  In education technology, auto grading of free text has long been a sought-for solution,
  particulary in large-scale assessments and technical subjects like Computer Science , where
  assessments cannot be easily reduced to multiple choice questions. As the number of students enrolled in 
  a course increases per instructor and the curriculum becomes more demanding, educators face a mounting pressure
  to provide fair, consistant and fast feedback.\\

  This posses a need to try to automate the process of marking and giving feedback. In the
  state of art of education right now, your instructor grades your assessments and 4 years later 
  provides you with a feedback, as we can argue without proof that early feedback helps students 
  perform better in their studies. \\

At the same time, Large Language Models(LLMs) like GPT-4, Claude, and open-source alternative
such as LLaMA and DeepSeek are proving to be not just the conversational tools but
they are capable evaluators, summarizers, and even tutors. Their impressive capacity
for natural Language understanding and generation raises an exciting possibility:

Can these models be harnessed to grade student responses automatically and meaningfully?
--and perhaps even provide personalized feedback that guides learning?


\subsection{Purpose of this review}
% \subsection{This is a Subsection}
% \subsubsection{This is a subsubsection}
The aim of this review is to syntesize and critically examine recent research
on the use of LLMs for autograding and feedback generation in educational contexts.
We shall look at how these models have been applied to assess student submissions,
what techniques have been used to improve reliability and fairness, and how feedback
mechanisms are integrated to support and elevate student learning.\\
By comparing methodologies, models and findings we aim to infer what is 
working, what remains challenging, and where there's space for improvement.
This should ultimately inform how we design our own LLM-based grading system 
for technical subjects, one that minimizes educators workload while enhancing the feedback experienca 



\subsection{Brief overview of the key themes}
Across the literature I have examimed, several key themes and technical strategies emerge.

\begin{table}[h!]
  \centering
  \begin{tabular}{|p{5cm}|p{12cm}|}
  \hline
  \textbf{Theme} & \textbf{Approaches / Observations} \\
  \hline
  Grading Accuracy & LLMs like GPT-4 and fine-tuned BERT models show strong agreement with human graders. \\
  \hline
  Feedback Generation & Models like BeGrading and GPT-4 in science writing generate formative feedback students find useful. \\
  \hline
  Explainability & SHAP explanations and linguistic feature analysis offer insight into deep model decisions. \\
  \hline
  Data Efficiency & Active learning (uncertainty, topology, hybrid methods) helps reduce labeling needs. \\
  \hline
  Prompt Engineering & Careful prompt design significantly improves LLM grading consistency and relevance. \\
  \hline
  Peer/Human-in-the-loop & Hybrid systems using peer grading or instructor scaffolding improve fairness and training. \\
  \hline
  Open-Source Model Potential & LLaMA 2 and Falcon perform comparably to commercial models in bioinformatics grading tasks. \\
  \hline
  \end{tabular}
  \caption{Themes and Observations in AI-based Educational Assessment}
  \label{tab:grading_ai_themes}
  \end{table}
We shall explore the above themes in deapth next section, critically comparing the approaches, results
and proposed direction of each study.
\chapter{Body}
\section{Body.}
\subsection{From Black-Box to transparent graders}
\textit{We trust teachers because when we ask them why, they give us a sensible responses; can we do the same for machines?}\\
In the early days of Automated Essay Scoring(AES), the focus was primaraly on \textbf{Accuracy}: this means
really we were answering the question could a machine assign the same score a human would?
Systems like e-raters while wildy adopted, remained blacked boxes. What this essentially means is 
it can produce grades without justification. Without clear reasoning for scores, students were left with 
limited guidance for improvement, and the instructors struggled to verify the fairness and reliability of these systems.\\
Mok (2023) addressed this limitation by developing a deep learning AES model based on a Multi-Layer Perceptron (MLP) architecture, trained on the ASAP Grade 7 narrative writing dataset. The model predicted rubric-level scores across four dimensions — ideas, organization, style, and conventions. Importantly, this work introduced SHAP (SHapley Additive exPlanations) to improve interpretability. SHAP provided both global (dataset-wide) and local (individual essay) feature attributions, making it possible to understand how linguistic features (such as lexical diversity, grammatical structure, or cohesion) contributed to predicted scores.\\
The model extracted over 1,500 linguistic indices using the SALAT toolkit, representing textual characteristics across five linguistic domains. Through regularization (Lasso and Ridge), the most informative features were selected. The SHAP analysis revealed that longer essays with richer lexical variety, clearer cohesion, and fewer grammatical errors were consistently rated higher by both human and automated systems. This provided not just a mechanism for score prediction, but a foundation for pedagogically meaningful feedback.\\
Despite these advances, a challenge remained: although SHAP offered detailed attribution scores, the model lacked the ability to transform those insights into actionable, human-readable feedback for students. Mok (2023) acknowledges that while these explanations are useful to researchers and educators, additional work is needed to make them accessible and educationally impactful for learners.\\
This work forms a foundation for more recent research that aims to not only predict scores with transparency, but to deliver feedback that supports learning. In doing so, it repositions the role of the AES system — from a passive grader to an active participant in the learning process.
The table below summarizes this study.
\begin{table}[h!]
  \centering
  \begin{tabular}{|p{4.5cm}|p{10.5cm}|}
  \hline
  \textbf{Aspect} & \textbf{Details} \\
  \hline
  Study Objective & Develop an explainable deep learning model to score essays and highlight linguistic features affecting grades \\
  \hline
  Dataset Used & ASAP Grade 7 Narrative Essays (1,567 essays, scored on 0–3 scale across four rubrics) \\
  \hline
  Model Type & Multi-Layer Perceptron (MLP) with 2–6 hidden layers \\
  \hline
  Input Features & 1,592 linguistic features extracted via SALAT toolkit (e.g., cohesion, syntax, grammar) \\
  \hline
  Feature Selection & Lasso \& Ridge Regression; low-variance filtering \\
  \hline
  Explainability Technique & SHAP (KernelSHAP, DeepSHAP, GradientSHAP) \\
  \hline
  Top Features for High Scores & 
  \begin{itemize}
    \item Essay length
    \item Lexical diversity
    \item Sentence complexity
    \item Cohesive ties
    \item Correct grammar usage
  \end{itemize} \\
  \hline
  Key Strength & SHAP provides global and local explanations; reveals what textual features matter most \\
  \hline
  Main Limitation & Explanations not yet in student-readable, feedback-ready form \\
  \hline
  \end{tabular}
  \caption{Summary of Mok (2023) — Explainable AES with SHAP Analysis}
  \label{tab:mok2023_summary}
  \end{table}
  
  \subsection{Feedback Generation and Instructional Support via Large Language Models}
  Recent research has moved beyond simply using LLMs to assign grades, exploring their potential to deliver personalized feedback — a critical element in promoting student learning. Feedback not only clarifies why a response was graded a certain way but also guides students on how to improve. Studies now show that, with appropriate guidance, LLMs can simulate this instructional role.

  In one study, Poličar et al. (2025) applied LLMs in a bioinformatics course to grade written assignments and provide feedback. Six models were tested, including commercial options (GPT-4, GPT-3.5, Claude) and open-source alternatives (LLaMA 2, Falcon 40B). Using a blind evaluation, students rated feedback without knowing its source. Notably, feedback from LLMs was often rated as helpful as that from human teaching assistants. Open-source models performed comparably to commercial ones, suggesting viable, privacy-conscious pathways for educational deployment.
  
  This success relied on prompt engineering: feeding the model structured inputs like the correct answer, grading rubrics, and example responses. This enabled models to produce relevant, contextualized feedback without the need for fine-tuning.
  
  Similarly, Impey et al. (2024) investigated GPT-4's grading capabilities in science writing. Given a rubric and model answer, GPT-4 reliably scored and explained its decisions, demonstrating instructional potential across domains.
  
  Both studies emphasize that when guided properly, LLMs can offer explanations and suggestions — not just evaluations. However, limitations remain. LLMs may overlook subtle domain-specific issues, and the quality of their feedback heavily depends on prompt clarity and structure.

  \begin{table}[h!]
    \centering
    \begin{tabular}{|p{3.5cm}|p{6.5cm}|p{6.5cm}|}
    \hline
    \textbf{Aspect} & \textbf{Poličar et al. (2025)} & \textbf{Impey et al. (2024)} \\
    \hline
    Educational Context & Bioinformatics course (university-level, >100 students) & MOOCs in astronomy and astrobiology (Coursera platform) \\
    \hline
    Task Type & Written explanations of data analyses in assignments & Short essay-style answers to open-ended science questions \\
    \hline
    Models Evaluated & GPT-4, GPT-3.5, Claude, LLaMA 2 (13B, 70B), Falcon 40B & GPT-4 \\
    \hline
    Grading Setup & Blind evaluation comparing LLM feedback to human TA feedback & Comparison to instructor grades; GPT-4 guided with rubric and model answer \\
    \hline
    Feedback Approach & Prompt engineering using rubrics, sample answers, and solutions & Rubric-based grading and rubric generation by the model \\
    \hline
    Key Findings & LLM feedback rated equal to human TAs; open-source models performed well & GPT-4 grading reliable and more consistent than peer grading \\
    \hline
    Limitations & Occasional domain-specific misses; dependent on prompt quality & Performance tied closely to rubric design; limited subject scope \\
    \hline
    \end{tabular}
    \caption{Summary of Studies on LLM-Based Feedback Generation}
    \label{tab:llm_feedback_studies}
    \end{table}
    













\subsection{A Subsection about Citation Style}
Citations are important. Citation style for Computer Science is:
\begin{itemize}
\item When used in the text, use the authors with the date in brackets:\\ \citet{klein17} say very important things.
\item When used as a reference after a face, put everything in brackets:\\ Import things are true \citep{klein17}.
\end{itemize}

\subsection{Compiling}
Remember to compile multiple times to resolve references. Usually:
\begin{verbatim}
pdflatex file.tex
bibtex file
pdflatex file.tex
pdflatex file.tex
\end{verbatim}


\chapter{Floats}
\LaTeX\ decides how to place images. It also does the referencing for you as seen in \Cref{fig:thing1}. If you have subimages, they should have their own captions and labels -- look into the subfig or subfigure packages.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.1\linewidth]{images/wits}
	\caption{This is an image}
	\label{fig:thing1}
\end{figure}

Figure captions are at the bottom. Table title are at the top of the table as seen in \Vref{tab:tab1}. There is a package called BookTabs which is \textit{way} better for tables and you should learn how to use that instead.

\begin{table}[p]
	\centering
	\caption{Table Name}
	\label{tab:tab1}
\begin{tabular}{cc}
	\hline
	Col1 & Col2\\
	\hline\hline 
	R0,C0 & R0,C1 \\ 
	R1,C0 & R1,C1 \\ 
	\hline
\end{tabular} 
\end{table}

Usually let \LaTeX\ handle the placement of floats unless you \textit{really} need to force it to do something else. The \texttt{float} package used above allows you to use \texttt{H} as the placement which means \textit{here and only here}. When using the float package, the placement options are:
\begin{enumerate}
\item h -- a gentle nudge to place it here if possible
\item t -- top of a page
\item b -- bottom of a page
\item H -- here and only here, do not move it at all
\item p -- on its own page
\end{enumerate}


\chapter{Some Referencing Tricks}
CleverRef and VarioRef are helpful:
\begin{itemize}
	\item Normal Ref: See Figure \ref{fig:thing1}
	\item CleverRef: See \Cref{fig:thing1} and \Cref{tab:tab1}
	\item CleverRef+VarioRef: See \Vref{fig:thing1} and \Vref{tab:tab1}
\end{itemize}

\chapter{IDE/Editors}
Overleaf has a great online editor for latex. Use it. 

\appendix
\chapter{Extra Stuff}\label{app:extra}
\section{What is an appendix?}\label{app:whatis}

An appendix is useful when there is information that you need to include, but breaks the flow of your document, e.g. a large number of figures/tables may need to be shown, but maybe only one needs to be in the text and the rest are just included for completeness.

\nocite{*}

\bibliography{references}\addcontentsline{toc}{chapter}{References}
\end{document}
